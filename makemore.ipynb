{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: learning from names in names.txt, and creating more names.\n",
    "\n",
    "This is a 'character-level' language model, where we try to predict the next character.\n",
    "\n",
    "\n",
    "These are examples of language models:\n",
    "\n",
    "Bigram (one character predicts the next one with a lookup table of counts)\n",
    "\n",
    "MLP, following Bengio et al. 2003\n",
    "\n",
    "CNN, following DeepMind WaveNet 2016 (in progress...)\n",
    "\n",
    "RNN, following Mikolov et al. 2010\n",
    "\n",
    "LSTM, following Graves et al. 2014\n",
    "\n",
    "GRU, following Kyunghyun Cho et al. 2014\n",
    "\n",
    "Transformer, following Vaswani et al. 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32033,\n",
       " ['emma',\n",
       "  'olivia',\n",
       "  'ava',\n",
       "  'isabella',\n",
       "  'sophia',\n",
       "  'charlotte',\n",
       "  'mia',\n",
       "  'amelia',\n",
       "  'harper',\n",
       "  'evelyn'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names), names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('an', 'muhammadibrahim')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(names, key=lambda x: len(x)), max(names, key=lambda x: len(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biagram\n",
    "\n",
    "Idea, we use i-th character, to predict (i + 1)th\n",
    "\n",
    "Essentially, no info about [0, i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "START_CH = '<S>' # this means the name started\n",
    "END_CH   = '<E>' # this means the name started\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '<E>'), 6763),\n",
       " (('a', '<E>'), 6640),\n",
       " (('a', 'n'), 5438),\n",
       " (('<S>', 'a'), 4410),\n",
       " (('e', '<E>'), 3983),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', 'l'), 3248),\n",
       " (('r', 'i'), 3033),\n",
       " (('n', 'a'), 2977),\n",
       " (('<S>', 'k'), 2963),\n",
       " (('l', 'e'), 2921),\n",
       " (('e', 'n'), 2675),\n",
       " (('l', 'a'), 2623),\n",
       " (('m', 'a'), 2590),\n",
       " (('<S>', 'm'), 2538),\n",
       " (('a', 'l'), 2528),\n",
       " (('i', '<E>'), 2489),\n",
       " (('l', 'i'), 2480),\n",
       " (('i', 'a'), 2445),\n",
       " (('<S>', 'j'), 2422),\n",
       " (('o', 'n'), 2411),\n",
       " (('h', '<E>'), 2409),\n",
       " (('r', 'a'), 2356),\n",
       " (('a', 'h'), 2332),\n",
       " (('h', 'a'), 2244),\n",
       " (('y', 'a'), 2143),\n",
       " (('i', 'n'), 2126),\n",
       " (('<S>', 's'), 2055),\n",
       " (('a', 'y'), 2050),\n",
       " (('y', '<E>'), 2007),\n",
       " (('e', 'r'), 1958),\n",
       " (('n', 'n'), 1906),\n",
       " (('y', 'n'), 1826),\n",
       " (('k', 'a'), 1731),\n",
       " (('n', 'i'), 1725),\n",
       " (('r', 'e'), 1697),\n",
       " (('<S>', 'd'), 1690),\n",
       " (('i', 'e'), 1653),\n",
       " (('a', 'i'), 1650),\n",
       " (('<S>', 'r'), 1639),\n",
       " (('a', 'm'), 1634),\n",
       " (('l', 'y'), 1588),\n",
       " (('<S>', 'l'), 1572),\n",
       " (('<S>', 'c'), 1542),\n",
       " (('<S>', 'e'), 1531),\n",
       " (('j', 'a'), 1473),\n",
       " (('r', '<E>'), 1377),\n",
       " (('n', 'e'), 1359),\n",
       " (('l', 'l'), 1345),\n",
       " (('i', 'l'), 1345),\n",
       " (('i', 's'), 1316),\n",
       " (('l', '<E>'), 1314),\n",
       " (('<S>', 't'), 1308),\n",
       " (('<S>', 'b'), 1306),\n",
       " (('d', 'a'), 1303),\n",
       " (('s', 'h'), 1285),\n",
       " (('d', 'e'), 1283),\n",
       " (('e', 'e'), 1271),\n",
       " (('m', 'i'), 1256),\n",
       " (('s', 'a'), 1201),\n",
       " (('s', '<E>'), 1169),\n",
       " (('<S>', 'n'), 1146),\n",
       " (('a', 's'), 1118),\n",
       " (('y', 'l'), 1104),\n",
       " (('e', 'y'), 1070),\n",
       " (('o', 'r'), 1059),\n",
       " (('a', 'd'), 1042),\n",
       " (('t', 'a'), 1027),\n",
       " (('<S>', 'z'), 929),\n",
       " (('v', 'i'), 911),\n",
       " (('k', 'e'), 895),\n",
       " (('s', 'e'), 884),\n",
       " (('<S>', 'h'), 874),\n",
       " (('r', 'o'), 869),\n",
       " (('e', 's'), 861),\n",
       " (('z', 'a'), 860),\n",
       " (('o', '<E>'), 855),\n",
       " (('i', 'r'), 849),\n",
       " (('b', 'r'), 842),\n",
       " (('a', 'v'), 834),\n",
       " (('m', 'e'), 818),\n",
       " (('e', 'i'), 818),\n",
       " (('c', 'a'), 815),\n",
       " (('i', 'y'), 779),\n",
       " (('r', 'y'), 773),\n",
       " (('e', 'm'), 769),\n",
       " (('s', 't'), 765),\n",
       " (('h', 'i'), 729),\n",
       " (('t', 'e'), 716),\n",
       " (('n', 'd'), 704),\n",
       " (('l', 'o'), 692),\n",
       " (('a', 'e'), 692),\n",
       " (('a', 't'), 687),\n",
       " (('s', 'i'), 684),\n",
       " (('e', 'a'), 679),\n",
       " (('d', 'i'), 674),\n",
       " (('h', 'e'), 674),\n",
       " (('<S>', 'g'), 669),\n",
       " (('t', 'o'), 667),\n",
       " (('c', 'h'), 664),\n",
       " (('b', 'e'), 655),\n",
       " (('t', 'h'), 647),\n",
       " (('v', 'a'), 642),\n",
       " (('o', 'l'), 619),\n",
       " (('<S>', 'i'), 591),\n",
       " (('i', 'o'), 588),\n",
       " (('e', 't'), 580),\n",
       " (('v', 'e'), 568),\n",
       " (('a', 'k'), 568),\n",
       " (('a', 'a'), 556),\n",
       " (('c', 'e'), 551),\n",
       " (('a', 'b'), 541),\n",
       " (('i', 't'), 541),\n",
       " (('<S>', 'y'), 535),\n",
       " (('t', 'i'), 532),\n",
       " (('s', 'o'), 531),\n",
       " (('m', '<E>'), 516),\n",
       " (('d', '<E>'), 516),\n",
       " (('<S>', 'p'), 515),\n",
       " (('i', 'c'), 509),\n",
       " (('k', 'i'), 509),\n",
       " (('o', 's'), 504),\n",
       " (('n', 'o'), 496),\n",
       " (('t', '<E>'), 483),\n",
       " (('j', 'o'), 479),\n",
       " (('u', 's'), 474),\n",
       " (('a', 'c'), 470),\n",
       " (('n', 'y'), 465),\n",
       " (('e', 'v'), 463),\n",
       " (('s', 's'), 461),\n",
       " (('m', 'o'), 452),\n",
       " (('i', 'k'), 445),\n",
       " (('n', 't'), 443),\n",
       " (('i', 'd'), 440),\n",
       " (('j', 'e'), 440),\n",
       " (('a', 'z'), 435),\n",
       " (('i', 'g'), 428),\n",
       " (('i', 'm'), 427),\n",
       " (('r', 'r'), 425),\n",
       " (('d', 'r'), 424),\n",
       " (('<S>', 'f'), 417),\n",
       " (('u', 'r'), 414),\n",
       " (('r', 'l'), 413),\n",
       " (('y', 's'), 401),\n",
       " (('<S>', 'o'), 394),\n",
       " (('e', 'd'), 384),\n",
       " (('a', 'u'), 381),\n",
       " (('c', 'o'), 380),\n",
       " (('k', 'y'), 379),\n",
       " (('d', 'o'), 378),\n",
       " (('<S>', 'v'), 376),\n",
       " (('t', 't'), 374),\n",
       " (('z', 'e'), 373),\n",
       " (('z', 'i'), 364),\n",
       " (('k', '<E>'), 363),\n",
       " (('g', 'h'), 360),\n",
       " (('t', 'r'), 352),\n",
       " (('k', 'o'), 344),\n",
       " (('t', 'y'), 341),\n",
       " (('g', 'e'), 334),\n",
       " (('g', 'a'), 330),\n",
       " (('l', 'u'), 324),\n",
       " (('b', 'a'), 321),\n",
       " (('d', 'y'), 317),\n",
       " (('c', 'k'), 316),\n",
       " (('<S>', 'w'), 307),\n",
       " (('k', 'h'), 307),\n",
       " (('u', 'l'), 301),\n",
       " (('y', 'e'), 301),\n",
       " (('y', 'r'), 291),\n",
       " (('m', 'y'), 287),\n",
       " (('h', 'o'), 287),\n",
       " (('w', 'a'), 280),\n",
       " (('s', 'l'), 279),\n",
       " (('n', 's'), 278),\n",
       " (('i', 'z'), 277),\n",
       " (('u', 'n'), 275),\n",
       " (('o', 'u'), 275),\n",
       " (('n', 'g'), 273),\n",
       " (('y', 'd'), 272),\n",
       " (('c', 'i'), 271),\n",
       " (('y', 'o'), 271),\n",
       " (('i', 'v'), 269),\n",
       " (('e', 'o'), 269),\n",
       " (('o', 'm'), 261),\n",
       " (('r', 'u'), 252),\n",
       " (('f', 'a'), 242),\n",
       " (('b', 'i'), 217),\n",
       " (('s', 'y'), 215),\n",
       " (('n', 'c'), 213),\n",
       " (('h', 'y'), 213),\n",
       " (('p', 'a'), 209),\n",
       " (('r', 't'), 208),\n",
       " (('q', 'u'), 206),\n",
       " (('p', 'h'), 204),\n",
       " (('h', 'r'), 204),\n",
       " (('j', 'u'), 202),\n",
       " (('g', 'r'), 201),\n",
       " (('p', 'e'), 197),\n",
       " (('n', 'l'), 195),\n",
       " (('y', 'i'), 192),\n",
       " (('g', 'i'), 190),\n",
       " (('o', 'd'), 190),\n",
       " (('r', 's'), 190),\n",
       " (('r', 'd'), 187),\n",
       " (('h', 'l'), 185),\n",
       " (('s', 'u'), 185),\n",
       " (('a', 'x'), 182),\n",
       " (('e', 'z'), 181),\n",
       " (('e', 'k'), 178),\n",
       " (('o', 'v'), 176),\n",
       " (('a', 'j'), 175),\n",
       " (('o', 'h'), 171),\n",
       " (('u', 'e'), 169),\n",
       " (('m', 'm'), 168),\n",
       " (('a', 'g'), 168),\n",
       " (('h', 'u'), 166),\n",
       " (('x', '<E>'), 164),\n",
       " (('u', 'a'), 163),\n",
       " (('r', 'm'), 162),\n",
       " (('a', 'w'), 161),\n",
       " (('f', 'i'), 160),\n",
       " (('z', '<E>'), 160),\n",
       " (('u', '<E>'), 155),\n",
       " (('u', 'm'), 154),\n",
       " (('e', 'c'), 153),\n",
       " (('v', 'o'), 153),\n",
       " (('e', 'h'), 152),\n",
       " (('p', 'r'), 151),\n",
       " (('d', 'd'), 149),\n",
       " (('o', 'a'), 149),\n",
       " (('w', 'e'), 149),\n",
       " (('w', 'i'), 148),\n",
       " (('y', 'm'), 148),\n",
       " (('z', 'y'), 147),\n",
       " (('n', 'z'), 145),\n",
       " (('y', 'u'), 141),\n",
       " (('r', 'n'), 140),\n",
       " (('o', 'b'), 140),\n",
       " (('k', 'l'), 139),\n",
       " (('m', 'u'), 139),\n",
       " (('l', 'd'), 138),\n",
       " (('h', 'n'), 138),\n",
       " (('u', 'd'), 136),\n",
       " (('<S>', 'x'), 134),\n",
       " (('t', 'l'), 134),\n",
       " (('a', 'f'), 134),\n",
       " (('o', 'e'), 132),\n",
       " (('e', 'x'), 132),\n",
       " (('e', 'g'), 125),\n",
       " (('f', 'e'), 123),\n",
       " (('z', 'l'), 123),\n",
       " (('u', 'i'), 121),\n",
       " (('v', 'y'), 121),\n",
       " (('e', 'b'), 121),\n",
       " (('r', 'h'), 121),\n",
       " (('j', 'i'), 119),\n",
       " (('o', 't'), 118),\n",
       " (('d', 'h'), 118),\n",
       " (('h', 'm'), 117),\n",
       " (('c', 'l'), 116),\n",
       " (('o', 'o'), 115),\n",
       " (('y', 'c'), 115),\n",
       " (('o', 'w'), 114),\n",
       " (('o', 'c'), 114),\n",
       " (('f', 'r'), 114),\n",
       " (('b', '<E>'), 114),\n",
       " (('m', 'b'), 112),\n",
       " (('z', 'o'), 110),\n",
       " (('i', 'b'), 110),\n",
       " (('i', 'u'), 109),\n",
       " (('k', 'r'), 109),\n",
       " (('g', '<E>'), 108),\n",
       " (('y', 'v'), 106),\n",
       " (('t', 'z'), 105),\n",
       " (('b', 'o'), 105),\n",
       " (('c', 'y'), 104),\n",
       " (('y', 't'), 104),\n",
       " (('u', 'b'), 103),\n",
       " (('u', 'c'), 103),\n",
       " (('x', 'a'), 103),\n",
       " (('b', 'l'), 103),\n",
       " (('o', 'y'), 103),\n",
       " (('x', 'i'), 102),\n",
       " (('i', 'f'), 101),\n",
       " (('r', 'c'), 99),\n",
       " (('c', '<E>'), 97),\n",
       " (('m', 'r'), 97),\n",
       " (('n', 'u'), 96),\n",
       " (('o', 'p'), 95),\n",
       " (('i', 'h'), 95),\n",
       " (('k', 's'), 95),\n",
       " (('l', 's'), 94),\n",
       " (('u', 'k'), 93),\n",
       " (('<S>', 'q'), 92),\n",
       " (('d', 'u'), 92),\n",
       " (('s', 'm'), 90),\n",
       " (('r', 'k'), 90),\n",
       " (('i', 'x'), 89),\n",
       " (('v', '<E>'), 88),\n",
       " (('y', 'k'), 86),\n",
       " (('u', 'w'), 86),\n",
       " (('g', 'u'), 85),\n",
       " (('b', 'y'), 83),\n",
       " (('e', 'p'), 83),\n",
       " (('g', 'o'), 83),\n",
       " (('s', 'k'), 82),\n",
       " (('u', 't'), 82),\n",
       " (('a', 'p'), 82),\n",
       " (('e', 'f'), 82),\n",
       " (('i', 'i'), 82),\n",
       " (('r', 'v'), 80),\n",
       " (('f', '<E>'), 80),\n",
       " (('t', 'u'), 78),\n",
       " (('y', 'z'), 78),\n",
       " (('<S>', 'u'), 78),\n",
       " (('l', 't'), 77),\n",
       " (('r', 'g'), 76),\n",
       " (('c', 'r'), 76),\n",
       " (('i', 'j'), 76),\n",
       " (('w', 'y'), 73),\n",
       " (('z', 'u'), 73),\n",
       " (('l', 'v'), 72),\n",
       " (('h', 't'), 71),\n",
       " (('j', '<E>'), 71),\n",
       " (('x', 't'), 70),\n",
       " (('o', 'i'), 69),\n",
       " (('e', 'u'), 69),\n",
       " (('o', 'k'), 68),\n",
       " (('b', 'd'), 65),\n",
       " (('a', 'o'), 63),\n",
       " (('p', 'i'), 61),\n",
       " (('s', 'c'), 60),\n",
       " (('d', 'l'), 60),\n",
       " (('l', 'm'), 60),\n",
       " (('a', 'q'), 60),\n",
       " (('f', 'o'), 60),\n",
       " (('p', 'o'), 59),\n",
       " (('n', 'k'), 58),\n",
       " (('w', 'n'), 58),\n",
       " (('u', 'h'), 58),\n",
       " (('e', 'j'), 55),\n",
       " (('n', 'v'), 55),\n",
       " (('s', 'r'), 55),\n",
       " (('o', 'z'), 54),\n",
       " (('i', 'p'), 53),\n",
       " (('l', 'b'), 52),\n",
       " (('i', 'q'), 52),\n",
       " (('w', '<E>'), 51),\n",
       " (('m', 'c'), 51),\n",
       " (('s', 'p'), 51),\n",
       " (('e', 'w'), 50),\n",
       " (('k', 'u'), 50),\n",
       " (('v', 'r'), 48),\n",
       " (('u', 'g'), 47),\n",
       " (('o', 'x'), 45),\n",
       " (('u', 'z'), 45),\n",
       " (('z', 'z'), 45),\n",
       " (('j', 'h'), 45),\n",
       " (('b', 'u'), 45),\n",
       " (('o', 'g'), 44),\n",
       " (('n', 'r'), 44),\n",
       " (('f', 'f'), 44),\n",
       " (('n', 'j'), 44),\n",
       " (('z', 'h'), 43),\n",
       " (('c', 'c'), 42),\n",
       " (('r', 'b'), 41),\n",
       " (('x', 'o'), 41),\n",
       " (('b', 'h'), 41),\n",
       " (('p', 'p'), 39),\n",
       " (('x', 'l'), 39),\n",
       " (('h', 'v'), 39),\n",
       " (('b', 'b'), 38),\n",
       " (('m', 'p'), 38),\n",
       " (('x', 'x'), 38),\n",
       " (('u', 'v'), 37),\n",
       " (('x', 'e'), 36),\n",
       " (('w', 'o'), 36),\n",
       " (('c', 't'), 35),\n",
       " (('z', 'm'), 35),\n",
       " (('t', 's'), 35),\n",
       " (('m', 's'), 35),\n",
       " (('c', 'u'), 35),\n",
       " (('o', 'f'), 34),\n",
       " (('u', 'x'), 34),\n",
       " (('k', 'w'), 34),\n",
       " (('p', '<E>'), 33),\n",
       " (('g', 'l'), 32),\n",
       " (('z', 'r'), 32),\n",
       " (('d', 'n'), 31),\n",
       " (('g', 't'), 31),\n",
       " (('g', 'y'), 31),\n",
       " (('h', 's'), 31),\n",
       " (('x', 's'), 31),\n",
       " (('g', 's'), 30),\n",
       " (('x', 'y'), 30),\n",
       " (('y', 'g'), 30),\n",
       " (('d', 'm'), 30),\n",
       " (('d', 's'), 29),\n",
       " (('h', 'k'), 29),\n",
       " (('y', 'x'), 28),\n",
       " (('q', '<E>'), 28),\n",
       " (('g', 'n'), 27),\n",
       " (('y', 'b'), 27),\n",
       " (('g', 'w'), 26),\n",
       " (('n', 'h'), 26),\n",
       " (('k', 'n'), 26),\n",
       " (('g', 'g'), 25),\n",
       " (('d', 'g'), 25),\n",
       " (('l', 'c'), 25),\n",
       " (('r', 'j'), 25),\n",
       " (('w', 'u'), 25),\n",
       " (('l', 'k'), 24),\n",
       " (('m', 'd'), 24),\n",
       " (('s', 'w'), 24),\n",
       " (('s', 'n'), 24),\n",
       " (('h', 'd'), 24),\n",
       " (('w', 'h'), 23),\n",
       " (('y', 'j'), 23),\n",
       " (('y', 'y'), 23),\n",
       " (('r', 'z'), 23),\n",
       " (('d', 'w'), 23),\n",
       " (('w', 'r'), 22),\n",
       " (('t', 'n'), 22),\n",
       " (('l', 'f'), 22),\n",
       " (('y', 'h'), 22),\n",
       " (('r', 'w'), 21),\n",
       " (('s', 'b'), 21),\n",
       " (('m', 'n'), 20),\n",
       " (('f', 'l'), 20),\n",
       " (('w', 's'), 20),\n",
       " (('k', 'k'), 20),\n",
       " (('h', 'z'), 20),\n",
       " (('g', 'd'), 19),\n",
       " (('l', 'h'), 19),\n",
       " (('n', 'm'), 19),\n",
       " (('x', 'z'), 19),\n",
       " (('u', 'f'), 19),\n",
       " (('f', 't'), 18),\n",
       " (('l', 'r'), 18),\n",
       " (('p', 't'), 17),\n",
       " (('t', 'c'), 17),\n",
       " (('k', 't'), 17),\n",
       " (('d', 'v'), 17),\n",
       " (('u', 'p'), 16),\n",
       " (('p', 'l'), 16),\n",
       " (('l', 'w'), 16),\n",
       " (('p', 's'), 16),\n",
       " (('o', 'j'), 16),\n",
       " (('r', 'q'), 16),\n",
       " (('y', 'p'), 15),\n",
       " (('l', 'p'), 15),\n",
       " (('t', 'v'), 15),\n",
       " (('r', 'p'), 14),\n",
       " (('l', 'n'), 14),\n",
       " (('e', 'q'), 14),\n",
       " (('f', 'y'), 14),\n",
       " (('s', 'v'), 14),\n",
       " (('u', 'j'), 14),\n",
       " (('v', 'l'), 14),\n",
       " (('q', 'a'), 13),\n",
       " (('u', 'y'), 13),\n",
       " (('q', 'i'), 13),\n",
       " (('w', 'l'), 13),\n",
       " (('p', 'y'), 12),\n",
       " (('y', 'f'), 12),\n",
       " (('c', 'q'), 11),\n",
       " (('j', 'r'), 11),\n",
       " (('n', 'w'), 11),\n",
       " (('n', 'f'), 11),\n",
       " (('t', 'w'), 11),\n",
       " (('m', 'z'), 11),\n",
       " (('u', 'o'), 10),\n",
       " (('f', 'u'), 10),\n",
       " (('l', 'z'), 10),\n",
       " (('h', 'w'), 10),\n",
       " (('u', 'q'), 10),\n",
       " (('j', 'y'), 10),\n",
       " (('s', 'z'), 10),\n",
       " (('s', 'd'), 9),\n",
       " (('j', 'l'), 9),\n",
       " (('d', 'j'), 9),\n",
       " (('k', 'm'), 9),\n",
       " (('r', 'f'), 9),\n",
       " (('h', 'j'), 9),\n",
       " (('v', 'n'), 8),\n",
       " (('n', 'b'), 8),\n",
       " (('i', 'w'), 8),\n",
       " (('h', 'b'), 8),\n",
       " (('b', 's'), 8),\n",
       " (('w', 't'), 8),\n",
       " (('w', 'd'), 8),\n",
       " (('v', 'v'), 7),\n",
       " (('v', 'u'), 7),\n",
       " (('j', 's'), 7),\n",
       " (('m', 'j'), 7),\n",
       " (('f', 's'), 6),\n",
       " (('l', 'g'), 6),\n",
       " (('l', 'j'), 6),\n",
       " (('j', 'w'), 6),\n",
       " (('n', 'x'), 6),\n",
       " (('y', 'q'), 6),\n",
       " (('w', 'k'), 6),\n",
       " (('g', 'm'), 6),\n",
       " (('x', 'u'), 5),\n",
       " (('m', 'h'), 5),\n",
       " (('m', 'l'), 5),\n",
       " (('j', 'm'), 5),\n",
       " (('c', 's'), 5),\n",
       " (('j', 'v'), 5),\n",
       " (('n', 'p'), 5),\n",
       " (('d', 'f'), 5),\n",
       " (('x', 'd'), 5),\n",
       " (('z', 'b'), 4),\n",
       " (('f', 'n'), 4),\n",
       " (('x', 'c'), 4),\n",
       " (('m', 't'), 4),\n",
       " (('t', 'm'), 4),\n",
       " (('z', 'n'), 4),\n",
       " (('z', 't'), 4),\n",
       " (('p', 'u'), 4),\n",
       " (('c', 'z'), 4),\n",
       " (('b', 'n'), 4),\n",
       " (('z', 's'), 4),\n",
       " (('f', 'w'), 4),\n",
       " (('d', 't'), 4),\n",
       " (('j', 'd'), 4),\n",
       " (('j', 'c'), 4),\n",
       " (('y', 'w'), 4),\n",
       " (('v', 'k'), 3),\n",
       " (('x', 'w'), 3),\n",
       " (('t', 'j'), 3),\n",
       " (('c', 'j'), 3),\n",
       " (('q', 'w'), 3),\n",
       " (('g', 'b'), 3),\n",
       " (('o', 'q'), 3),\n",
       " (('r', 'x'), 3),\n",
       " (('d', 'c'), 3),\n",
       " (('g', 'j'), 3),\n",
       " (('x', 'f'), 3),\n",
       " (('z', 'w'), 3),\n",
       " (('d', 'k'), 3),\n",
       " (('u', 'u'), 3),\n",
       " (('m', 'v'), 3),\n",
       " (('c', 'x'), 3),\n",
       " (('l', 'q'), 3),\n",
       " (('p', 'b'), 2),\n",
       " (('t', 'g'), 2),\n",
       " (('q', 's'), 2),\n",
       " (('t', 'x'), 2),\n",
       " (('f', 'k'), 2),\n",
       " (('b', 't'), 2),\n",
       " (('j', 'n'), 2),\n",
       " (('k', 'c'), 2),\n",
       " (('z', 'k'), 2),\n",
       " (('s', 'j'), 2),\n",
       " (('s', 'f'), 2),\n",
       " (('z', 'j'), 2),\n",
       " (('n', 'q'), 2),\n",
       " (('f', 'z'), 2),\n",
       " (('h', 'g'), 2),\n",
       " (('w', 'w'), 2),\n",
       " (('k', 'j'), 2),\n",
       " (('j', 'k'), 2),\n",
       " (('w', 'm'), 2),\n",
       " (('z', 'c'), 2),\n",
       " (('z', 'v'), 2),\n",
       " (('w', 'f'), 2),\n",
       " (('q', 'm'), 2),\n",
       " (('k', 'z'), 2),\n",
       " (('j', 'j'), 2),\n",
       " (('z', 'p'), 2),\n",
       " (('j', 't'), 2),\n",
       " (('k', 'b'), 2),\n",
       " (('m', 'w'), 2),\n",
       " (('h', 'f'), 2),\n",
       " (('c', 'g'), 2),\n",
       " (('t', 'f'), 2),\n",
       " (('h', 'c'), 2),\n",
       " (('q', 'o'), 2),\n",
       " (('k', 'd'), 2),\n",
       " (('k', 'v'), 2),\n",
       " (('s', 'g'), 2),\n",
       " (('z', 'd'), 2),\n",
       " (('q', 'r'), 1),\n",
       " (('d', 'z'), 1),\n",
       " (('p', 'j'), 1),\n",
       " (('q', 'l'), 1),\n",
       " (('p', 'f'), 1),\n",
       " (('q', 'e'), 1),\n",
       " (('b', 'c'), 1),\n",
       " (('c', 'd'), 1),\n",
       " (('m', 'f'), 1),\n",
       " (('p', 'n'), 1),\n",
       " (('w', 'b'), 1),\n",
       " (('p', 'c'), 1),\n",
       " (('h', 'p'), 1),\n",
       " (('f', 'h'), 1),\n",
       " (('b', 'j'), 1),\n",
       " (('f', 'g'), 1),\n",
       " (('z', 'g'), 1),\n",
       " (('c', 'p'), 1),\n",
       " (('p', 'k'), 1),\n",
       " (('p', 'm'), 1),\n",
       " (('x', 'n'), 1),\n",
       " (('s', 'q'), 1),\n",
       " (('k', 'f'), 1),\n",
       " (('m', 'k'), 1),\n",
       " (('x', 'h'), 1),\n",
       " (('g', 'f'), 1),\n",
       " (('v', 'b'), 1),\n",
       " (('j', 'p'), 1),\n",
       " (('g', 'z'), 1),\n",
       " (('v', 'd'), 1),\n",
       " (('d', 'b'), 1),\n",
       " (('v', 'h'), 1),\n",
       " (('h', 'h'), 1),\n",
       " (('g', 'v'), 1),\n",
       " (('d', 'q'), 1),\n",
       " (('x', 'b'), 1),\n",
       " (('w', 'z'), 1),\n",
       " (('h', 'q'), 1),\n",
       " (('j', 'b'), 1),\n",
       " (('x', 'm'), 1),\n",
       " (('w', 'g'), 1),\n",
       " (('t', 'b'), 1),\n",
       " (('z', 'x'), 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a basic implementation with python dictionaries\n",
    "# we will look at the frequency of each paris\n",
    "\n",
    "biagram_dict = {}\n",
    "for n in names:\n",
    "    chs = [START_CH] + list(n) + [END_CH]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        biagram = (ch1, ch2)\n",
    "        biagram_dict[biagram] = biagram_dict.get(biagram, 0) + 1\n",
    "\n",
    "# sort dict by value\n",
    "sorted(biagram_dict.items(), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use pytorch API for implementation as well\n",
    "# idea: each element of a matrix will show the count for a given row and column\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<S>': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '<E>': 27}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_alphabet = 26 + 2 # 2 for start and end\n",
    "N = torch.zeros((size_of_alphabet, size_of_alphabet), dtype=torch.int32)\n",
    "\n",
    "# creating a lookup table\n",
    "chars = [START_CH] + sorted(list(set(''.join(names)))) + [END_CH]\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filling matrix based on frequency\n",
    "\n",
    "for n in names:\n",
    "    chs = [START_CH] + list(n) + [END_CH]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1\n",
    "\n",
    "# now that we have this matrix, we just have to sample from it.\n",
    "# randomly start a new character, and randomly pick the next one based on probabilities in this matric, etc\n",
    "\n",
    "# creating a probability matrix\n",
    "# in practice, it's recommended to add a small number (like 1) to avoid zeros.\n",
    "P = (N + 1).float()\n",
    "P /= P.sum(1, keepdim=True)\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.2019e-06, 1.3765e-01, 4.0767e-02, 4.8138e-02, 5.2757e-02, 4.7795e-02,\n",
       "         1.3025e-02, 2.0885e-02, 2.7284e-02, 1.8451e-02, 7.5604e-02, 9.2484e-02,\n",
       "         4.9074e-02, 7.9224e-02, 3.5773e-02, 1.2308e-02, 1.6079e-02, 2.8766e-03,\n",
       "         5.1165e-02, 6.4149e-02, 4.0835e-02, 2.4397e-03, 1.1741e-02, 9.5870e-03,\n",
       "         4.1875e-03, 1.6708e-02, 2.9000e-02, 5.2019e-06]),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0,:], P[0,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['junidedianasa',\n",
       " 'jush',\n",
       " 'ay',\n",
       " 'a',\n",
       " 'nn',\n",
       " 'kohin',\n",
       " 'toli',\n",
       " 'asate',\n",
       " 'madahn',\n",
       " 'auran']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word creation\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "def get_new_names(n):\n",
    "    names = []\n",
    "    for _ in range(n):\n",
    "        new_name = [START_CH]\n",
    "        while True:\n",
    "            idx1 = stoi[new_name[-1]] # index of the last character\n",
    "            p = P[idx1,:]\n",
    "            idx2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "            new_ch = itos[idx2]\n",
    "            if new_ch == END_CH:\n",
    "                break\n",
    "            new_name.append(new_ch)\n",
    "        names.append(''.join(new_name[1:]))\n",
    "    return names\n",
    "\n",
    "get_new_names(10) # really terrible algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of likelihood, log likelihood, and nll\n",
    "\n",
    "loglikelihood = 0.0\n",
    "items = 0\n",
    "for n in ['aliqj']:\n",
    "    chs = [START_CH] + list(n) + [END_CH]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        idx1, idx2 = stoi[ch1], stoi[ch2]\n",
    "        prob = P[idx1, idx2]\n",
    "        loglikelihood += torch.log(prob)\n",
    "        items += 1\n",
    "        print(f\"token={ch1, ch2}, {prob=:.3f}\")\n",
    "\n",
    "print(f\"{loglikelihood=},  nll={-loglikelihood}, lossfunc={-loglikelihood / items}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we can formulate our problem like an optimization one: \n",
    "# (1) data\n",
    "# (2) parameters\n",
    "# (3) loss function\n",
    "\n",
    "# data is obvious: list of names\n",
    "# parameters are elements in probability matrix, P\n",
    "# the loss function to use here is nll (negative log likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theoretically, we could find P(i, j) by minimizing loss function using a minimizer, such as neural net.\n",
    "# model:\n",
    "#   input: takes a character as input (first character)\n",
    "#   output: probability of having a specific character next\n",
    "# we will build 27 of these models so that for a given input, it a list of probabilities for all characters\n",
    "# example:\n",
    "# xs = ['a']\n",
    "# ys = [0.1, 0.3, ..., 0.5]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: craeting training set of biagrams (x, y)\n",
    "# requires encoding\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for n in names:\n",
    "    chs = [START_CH] + list(n) + [END_CH]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146]), torch.Size([228146]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 28]),\n",
       " tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2: one-hot encoding.\n",
    "# xs integer values are not meaningful.\n",
    "# we need to transform it to dummy variables (aka one-hot encoding).\n",
    "# always make sure that inputs are float\n",
    "\n",
    "xenc = torch.nn.functional.one_hot(xs, num_classes=len(stoi)).float()\n",
    "xenc.shape, xenc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([28, 28]),\n",
       " tensor([[0.7081, 0.3542, 0.1054, 0.5996, 0.0904],\n",
       "         [0.8581, 0.3224, 0.5998, 0.1621, 0.3729],\n",
       "         [0.2275, 0.4577, 0.7701, 0.8770, 0.0389],\n",
       "         [0.5802, 0.7099, 0.5512, 0.6437, 0.6311],\n",
       "         [0.1589, 0.9281, 0.2298, 0.8939, 0.9322]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3: initializing weight tensor\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "W = torch.rand(len(stoi), len(stoi), requires_grad=True, generator=g) # there are 28 inputs, and 28 models (output neurons) that generates 28 prob.\n",
    "W.shape, W[0:5, 0:5]\n",
    "\n",
    "# each column contains weight for each of 28 models\n",
    "# xenc @ W[:,1] -> prob(second character = a | first character)\n",
    "# note: this is the most simple NN ever: no hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0 | loss=2.47235107421875\n",
      "iter=1 | loss=2.472313404083252\n",
      "iter=2 | loss=2.472275972366333\n",
      "iter=3 | loss=2.472238779067993\n",
      "iter=4 | loss=2.4722018241882324\n",
      "iter=5 | loss=2.4721648693084717\n",
      "iter=6 | loss=2.47212815284729\n",
      "iter=7 | loss=2.4720914363861084\n",
      "iter=8 | loss=2.472054958343506\n",
      "iter=9 | loss=2.4720184803009033\n",
      "iter=10 | loss=2.47198224067688\n",
      "iter=11 | loss=2.4719462394714355\n",
      "iter=12 | loss=2.471909999847412\n",
      "iter=13 | loss=2.471874475479126\n",
      "iter=14 | loss=2.4718387126922607\n",
      "iter=15 | loss=2.4718031883239746\n",
      "iter=16 | loss=2.4717679023742676\n",
      "iter=17 | loss=2.4717323780059814\n",
      "iter=18 | loss=2.4716973304748535\n",
      "iter=19 | loss=2.4716622829437256\n",
      "iter=20 | loss=2.4716274738311768\n",
      "iter=21 | loss=2.471592664718628\n",
      "iter=22 | loss=2.4715583324432373\n",
      "iter=23 | loss=2.4715237617492676\n",
      "iter=24 | loss=2.471489191055298\n",
      "iter=25 | loss=2.4714553356170654\n",
      "iter=26 | loss=2.4714207649230957\n",
      "iter=27 | loss=2.4713869094848633\n",
      "iter=28 | loss=2.47135329246521\n",
      "iter=29 | loss=2.4713196754455566\n",
      "iter=30 | loss=2.471285820007324\n",
      "iter=31 | loss=2.47125244140625\n",
      "iter=32 | loss=2.471219062805176\n",
      "iter=33 | loss=2.4711859226226807\n",
      "iter=34 | loss=2.4711527824401855\n",
      "iter=35 | loss=2.4711198806762695\n",
      "iter=36 | loss=2.4710869789123535\n",
      "iter=37 | loss=2.4710540771484375\n",
      "iter=38 | loss=2.471021890640259\n",
      "iter=39 | loss=2.470989227294922\n",
      "iter=40 | loss=2.470956802368164\n",
      "iter=41 | loss=2.4709248542785645\n",
      "iter=42 | loss=2.4708924293518066\n",
      "iter=43 | loss=2.470860719680786\n",
      "iter=44 | loss=2.4708287715911865\n",
      "iter=45 | loss=2.470797300338745\n",
      "iter=46 | loss=2.4707653522491455\n",
      "iter=47 | loss=2.470733642578125\n",
      "iter=48 | loss=2.4707024097442627\n",
      "iter=49 | loss=2.4706711769104004\n",
      "iter=50 | loss=2.470639944076538\n",
      "iter=51 | loss=2.470608711242676\n",
      "iter=52 | loss=2.4705779552459717\n",
      "iter=53 | loss=2.4705471992492676\n",
      "iter=54 | loss=2.4705164432525635\n",
      "iter=55 | loss=2.4704859256744385\n",
      "iter=56 | loss=2.4704549312591553\n",
      "iter=57 | loss=2.4704248905181885\n",
      "iter=58 | loss=2.4703946113586426\n",
      "iter=59 | loss=2.4703643321990967\n",
      "iter=60 | loss=2.47033429145813\n",
      "iter=61 | loss=2.470304489135742\n",
      "iter=62 | loss=2.4702744483947754\n",
      "iter=63 | loss=2.4702444076538086\n",
      "iter=64 | loss=2.470215082168579\n",
      "iter=65 | loss=2.4701855182647705\n",
      "iter=66 | loss=2.470156192779541\n",
      "iter=67 | loss=2.4701268672943115\n",
      "iter=68 | loss=2.470097541809082\n",
      "iter=69 | loss=2.4700684547424316\n",
      "iter=70 | loss=2.470039129257202\n",
      "iter=71 | loss=2.4700100421905518\n",
      "iter=72 | loss=2.4699814319610596\n",
      "iter=73 | loss=2.4699528217315674\n",
      "iter=74 | loss=2.469924211502075\n",
      "iter=75 | loss=2.469895362854004\n",
      "iter=76 | loss=2.46986722946167\n",
      "iter=77 | loss=2.469838857650757\n",
      "iter=78 | loss=2.4698104858398438\n",
      "iter=79 | loss=2.469782590866089\n",
      "iter=80 | loss=2.469754457473755\n",
      "iter=81 | loss=2.469726324081421\n",
      "iter=82 | loss=2.469698667526245\n",
      "iter=83 | loss=2.4696707725524902\n",
      "iter=84 | loss=2.4696433544158936\n",
      "iter=85 | loss=2.4696154594421387\n",
      "iter=86 | loss=2.469588279724121\n",
      "iter=87 | loss=2.4695606231689453\n",
      "iter=88 | loss=2.4695334434509277\n",
      "iter=89 | loss=2.46950626373291\n",
      "iter=90 | loss=2.4694793224334717\n",
      "iter=91 | loss=2.469452381134033\n",
      "iter=92 | loss=2.4694252014160156\n",
      "iter=93 | loss=2.4693984985351562\n",
      "iter=94 | loss=2.469371795654297\n",
      "iter=95 | loss=2.4693450927734375\n",
      "iter=96 | loss=2.469318389892578\n",
      "iter=97 | loss=2.469291925430298\n",
      "iter=98 | loss=2.469265937805176\n",
      "iter=99 | loss=2.4692394733428955\n",
      "iter=100 | loss=2.4692132472991943\n",
      "iter=101 | loss=2.4691872596740723\n",
      "iter=102 | loss=2.46916127204895\n",
      "iter=103 | loss=2.469135284423828\n",
      "iter=104 | loss=2.469109535217285\n",
      "iter=105 | loss=2.469083309173584\n",
      "iter=106 | loss=2.46905779838562\n",
      "iter=107 | loss=2.4690325260162354\n",
      "iter=108 | loss=2.4690070152282715\n",
      "iter=109 | loss=2.4689815044403076\n",
      "iter=110 | loss=2.4689557552337646\n",
      "iter=111 | loss=2.46893048286438\n",
      "iter=112 | loss=2.468905210494995\n",
      "iter=113 | loss=2.4688804149627686\n",
      "iter=114 | loss=2.468855381011963\n",
      "iter=115 | loss=2.4688308238983154\n",
      "iter=116 | loss=2.4688055515289307\n",
      "iter=117 | loss=2.468780994415283\n",
      "iter=118 | loss=2.4687561988830566\n",
      "iter=119 | loss=2.468731641769409\n",
      "iter=120 | loss=2.4687070846557617\n",
      "iter=121 | loss=2.4686825275421143\n",
      "iter=122 | loss=2.468658208847046\n",
      "iter=123 | loss=2.4686341285705566\n",
      "iter=124 | loss=2.4686098098754883\n",
      "iter=125 | loss=2.468585729598999\n",
      "iter=126 | loss=2.4685614109039307\n",
      "iter=127 | loss=2.4685378074645996\n",
      "iter=128 | loss=2.4685134887695312\n",
      "iter=129 | loss=2.468489646911621\n",
      "iter=130 | loss=2.468466281890869\n",
      "iter=131 | loss=2.468442440032959\n",
      "iter=132 | loss=2.468418836593628\n",
      "iter=133 | loss=2.468395233154297\n",
      "iter=134 | loss=2.468371868133545\n",
      "iter=135 | loss=2.468348503112793\n",
      "iter=136 | loss=2.468325138092041\n",
      "iter=137 | loss=2.468301773071289\n",
      "iter=138 | loss=2.4682788848876953\n",
      "iter=139 | loss=2.4682555198669434\n",
      "iter=140 | loss=2.4682326316833496\n",
      "iter=141 | loss=2.468209743499756\n",
      "iter=142 | loss=2.468186378479004\n",
      "iter=143 | loss=2.4681639671325684\n",
      "iter=144 | loss=2.4681413173675537\n",
      "iter=145 | loss=2.468118667602539\n",
      "iter=146 | loss=2.4680960178375244\n",
      "iter=147 | loss=2.468073606491089\n",
      "iter=148 | loss=2.4680511951446533\n",
      "iter=149 | loss=2.4680287837982178\n",
      "iter=150 | loss=2.4680066108703613\n",
      "iter=151 | loss=2.467984199523926\n",
      "iter=152 | loss=2.4679620265960693\n",
      "iter=153 | loss=2.467939853668213\n",
      "iter=154 | loss=2.4679176807403564\n",
      "iter=155 | loss=2.4678962230682373\n",
      "iter=156 | loss=2.46787428855896\n",
      "iter=157 | loss=2.4678523540496826\n",
      "iter=158 | loss=2.4678304195404053\n",
      "iter=159 | loss=2.467808723449707\n",
      "iter=160 | loss=2.467787265777588\n",
      "iter=161 | loss=2.4677653312683105\n",
      "iter=162 | loss=2.4677441120147705\n",
      "iter=163 | loss=2.4677224159240723\n",
      "iter=164 | loss=2.4677014350891113\n",
      "iter=165 | loss=2.467679977416992\n",
      "iter=166 | loss=2.467658519744873\n",
      "iter=167 | loss=2.467637538909912\n",
      "iter=168 | loss=2.4676167964935303\n",
      "iter=169 | loss=2.4675955772399902\n",
      "iter=170 | loss=2.4675745964050293\n",
      "iter=171 | loss=2.4675536155700684\n",
      "iter=172 | loss=2.4675328731536865\n",
      "iter=173 | loss=2.4675116539001465\n",
      "iter=174 | loss=2.4674911499023438\n",
      "iter=175 | loss=2.467470407485962\n",
      "iter=176 | loss=2.4674501419067383\n",
      "iter=177 | loss=2.4674293994903564\n",
      "iter=178 | loss=2.4674086570739746\n",
      "iter=179 | loss=2.467388391494751\n",
      "iter=180 | loss=2.4673678874969482\n",
      "iter=181 | loss=2.4673476219177246\n",
      "iter=182 | loss=2.467327356338501\n",
      "iter=183 | loss=2.4673073291778564\n",
      "iter=184 | loss=2.467287063598633\n",
      "iter=185 | loss=2.4672670364379883\n",
      "iter=186 | loss=2.4672470092773438\n",
      "iter=187 | loss=2.4672272205352783\n",
      "iter=188 | loss=2.467207431793213\n",
      "iter=189 | loss=2.4671874046325684\n",
      "iter=190 | loss=2.467167615890503\n",
      "iter=191 | loss=2.4671480655670166\n",
      "iter=192 | loss=2.4671285152435303\n",
      "iter=193 | loss=2.467108726501465\n",
      "iter=194 | loss=2.4670894145965576\n",
      "iter=195 | loss=2.4670698642730713\n",
      "iter=196 | loss=2.467050075531006\n",
      "iter=197 | loss=2.4670310020446777\n",
      "iter=198 | loss=2.4670114517211914\n",
      "iter=199 | loss=2.4669923782348633\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(200):\n",
    "\n",
    "    # step 4: forward propagation\n",
    "    logits = xenc @ W # we assume these are log of counts (softmax)\n",
    "    counts = logits.exp() # converting log count to count\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True) # normalization - shape is (#obs., 28)\n",
    "\n",
    "    # step 5: loss function calculation\n",
    "    lossfunc = -probs[torch.arange(len(probs)), ys].log().mean()\n",
    "    # lossfunc = -probs[torch.arange(len(probs)), ys].log().mean() + lambda * (W ** 2).mean()\n",
    "    print(f\"iter={iteration} | loss={lossfunc}\")\n",
    "\n",
    "    # step 6: backward propagation\n",
    "    W.grad = None # set grad zero\n",
    "    lossfunc.backward()\n",
    "\n",
    "    # step 7: update parameters\n",
    "    learning_rate = 10\n",
    "    W.data -= learning_rate * W.grad\n",
    "\n",
    "# we expect the loss converge to what we saw (2.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_name = [START_CH]\n",
    "idx1 = torch.tensor(stoi[new_name[-1]]) # index of the last character\n",
    "xenc1 = torch.nn.functional.one_hot(idx1, num_classes=len(stoi)).float()\n",
    "logit = xenc1 @ W\n",
    "count = logit.exp()\n",
    "p = count / count.sum()\n",
    "idx2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "new_ch = itos[idx2]\n",
    "new_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0733, 11.5699,  3.4218,  4.0414,  4.4299,  4.0125,  1.0877,  1.7495,\n",
       "         2.2878,  1.5447,  6.3514,  7.7715,  4.1201,  6.6559,  3.0018,  1.0273,\n",
       "         1.3451,  0.2444,  4.2960,  5.3880,  3.4271,  0.2103,  0.9800,  0.7986,\n",
       "         0.3469,  1.3976,  2.4321,  0.0730], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['junidedianasa',\n",
       " 'jush',\n",
       " 'ay',\n",
       " 'a',\n",
       " 'nn',\n",
       " 'kohin',\n",
       " 'toli',\n",
       " 'asate',\n",
       " 'madahn',\n",
       " 'auran']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word creation\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "def get_new_names_with_nn(n):\n",
    "    names = []\n",
    "    for _ in range(n):\n",
    "        new_name = [START_CH]\n",
    "        while True:\n",
    "            idx1 = torch.tensor(stoi[new_name[-1]]) # index of the last character\n",
    "            xenc1 = torch.nn.functional.one_hot(idx1, num_classes=len(stoi)).float()\n",
    "            logit = xenc1 @ W\n",
    "            count = logit.exp()\n",
    "            p = count / count.sum()\n",
    "            idx2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "            new_ch = itos[idx2]\n",
    "            if new_ch == END_CH:\n",
    "                break\n",
    "            new_name.append(new_ch)\n",
    "        names.append(''.join(new_name[1:]))\n",
    "    return names\n",
    "\n",
    "get_new_names_with_nn(10) # really terrible algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nll calculation concept\n",
    "\n",
    "# nlls = torch.zeros(len(probs))\n",
    "\n",
    "# for i in range(len(probs)):\n",
    "#     # i-th biagram\n",
    "#     x = xs[i].item()\n",
    "#     y = ys[i].item()\n",
    "#     predicted_p = probs[i, y]\n",
    "#     nlls[i] = -torch.log(predicted_p)\n",
    "\n",
    "# print(f\"nll = {nlls.mean().item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
